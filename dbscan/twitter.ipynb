{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be2d863",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "twitter = pd.read_csv('twitter.csv')\n",
    "print(twitter.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9976df24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Apply min-max scaling to twitter['timestamp'] column\n",
    "scaler_timestamp = MinMaxScaler()\n",
    "twitter['timestamp_scaled'] = scaler_timestamp.fit_transform(twitter[['timestamp']])\n",
    "\n",
    "# Apply min-max scaling to latitude and longitude\n",
    "scaler_latlong = MinMaxScaler()\n",
    "twitter[['lat_scaled', 'long_scaled']] = scaler_latlong.fit_transform(twitter[['latitude', 'longitude']])\n",
    "\n",
    "print(f\"Scaled Features Summary:\")\n",
    "print(f\"  Timestamp range: [{twitter['timestamp_scaled'].min()}, {twitter['timestamp_scaled'].max()}]\")\n",
    "print(f\"  Latitude range: [{twitter['lat_scaled'].min():.4f}, {twitter['lat_scaled'].max():.4f}]\")\n",
    "print(f\"  Longitude range: [{twitter['long_scaled'].min():.4f}, {twitter['long_scaled'].max():.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8a0145",
   "metadata": {},
   "source": [
    "## Twitter Dataset - DBSCAN with L2 (Euclidean) Distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac397bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features for clustering (scaled timestamp, lat, long - no response variable)\n",
    "twitter_features_scaled = twitter[['timestamp_scaled', 'lat_scaled', 'long_scaled']]\n",
    "\n",
    "print(f\"Twitter dataset size: {len(twitter_features_scaled)} samples\")\n",
    "print(\"Note: DBSCAN on large datasets is extremely memory-intensive\")\n",
    "\n",
    "# Use sampling for computational feasibility\n",
    "# DBSCAN has O(n²) complexity - full dataset would require too much memory\n",
    "sample_size = 100000  # Use 100k samples (adjust based on your memory)\n",
    "print(f\"\\n⚠️  Using sample of {sample_size} samples for DBSCAN\")\n",
    "print(\"(Running on full dataset would likely crash due to memory constraints)\")\n",
    "\n",
    "twitter_sample = twitter.sample(n=min(sample_size, len(twitter)), random_state=42)\n",
    "twitter_sample_idx = twitter_sample.index\n",
    "twitter_features_sample = twitter_features_scaled.loc[twitter_sample_idx]\n",
    "\n",
    "# Apply DBSCAN clustering\n",
    "# Using L2 (Euclidean) distance on scaled features\n",
    "twitter_dbscan = DBSCAN(\n",
    "    eps=0.15,             # Maximum distance between samples\n",
    "    min_samples=20,       # Minimum samples in a neighborhood\n",
    "    metric='euclidean',   # Use L2 (Euclidean) distance\n",
    "    n_jobs=-1             # Use all CPU cores\n",
    ")\n",
    "\n",
    "print(f\"\\nRunning DBSCAN on {len(twitter_features_sample)} samples...\")\n",
    "print(f\"Parameters: eps={twitter_dbscan.eps}, min_samples={twitter_dbscan.min_samples}\")\n",
    "\n",
    "# Fit and predict clusters on sample\n",
    "twitter_clusters_sample = twitter_dbscan.fit_predict(twitter_features_sample)\n",
    "\n",
    "# Initialize full dataset clusters as noise (-1)\n",
    "twitter_clusters = np.full(len(twitter), -1)\n",
    "# Assign cluster labels to sampled points\n",
    "twitter_clusters[twitter_sample_idx] = twitter_clusters_sample\n",
    "\n",
    "# Add cluster assignments to the dataframe\n",
    "twitter['dbscan_cluster'] = twitter_clusters\n",
    "\n",
    "# Display results\n",
    "print(\"\\n✅ DBSCAN completed!\")\n",
    "print(\"\\nTwitter DBSCAN Results (L2/Euclidean Distance):\")\n",
    "print(f\"  Sampled: {len(twitter_features_sample)} samples\")\n",
    "print(f\"  Clusters found: {len(set(twitter_clusters_sample)) - (1 if -1 in twitter_clusters_sample else 0)}\")\n",
    "print(f\"  Noise points in sample: {list(twitter_clusters_sample).count(-1)} ({list(twitter_clusters_sample).count(-1)/len(twitter_clusters_sample)*100:.2f}%)\")\n",
    "print(f\"  Unsampled points (marked as noise): {len(twitter) - len(twitter_features_sample)}\")\n",
    "print(f\"\\n  Cluster distribution (sample only - top 10 largest clusters):\")\n",
    "unique, counts = np.unique(twitter_clusters_sample, return_counts=True)\n",
    "sorted_indices = np.argsort(counts)[::-1][:10]\n",
    "for idx in sorted_indices:\n",
    "    cluster, count = unique[idx], counts[idx]\n",
    "    if cluster == -1:\n",
    "        print(f\"    Noise (-1): {count} samples\")\n",
    "    else:\n",
    "        print(f\"    Cluster {cluster}: {count} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b921e08",
   "metadata": {},
   "source": [
    "## Twitter Dataset - Compare with Ground Truth Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb81ac8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare DBSCAN clusters with actual timezone labels\n",
    "# Note: Only evaluating on the sampled points\n",
    "actual_timezones_sample = twitter.loc[twitter_sample_idx, 'timezone']\n",
    "\n",
    "# Calculate clustering evaluation metrics (on sample only)\n",
    "ari_twitter = adjusted_rand_score(actual_timezones_sample, twitter_clusters_sample)\n",
    "nmi_twitter = normalized_mutual_info_score(actual_timezones_sample, twitter_clusters_sample)\n",
    "homogeneity_tw, completeness_tw, v_measure_tw = homogeneity_completeness_v_measure(actual_timezones_sample, twitter_clusters_sample)\n",
    "\n",
    "print(\"Twitter Dataset - Clustering Evaluation (on sampled data):\")\n",
    "print(f\"  Sample size: {len(twitter_clusters_sample)} points\")\n",
    "print(f\"\\n  Adjusted Rand Index (ARI): {ari_twitter:.4f}\")\n",
    "print(f\"    (1.0 = perfect match, 0.0 = random, negative = worse than random)\")\n",
    "print(f\"\\n  Normalized Mutual Information (NMI): {nmi_twitter:.4f}\")\n",
    "print(f\"    (1.0 = perfect match, 0.0 = no mutual information)\")\n",
    "print(f\"\\n  Homogeneity: {homogeneity_tw:.4f}\")\n",
    "print(f\"    (1.0 = each cluster contains only members of a single class)\")\n",
    "print(f\"  Completeness: {completeness_tw:.4f}\")\n",
    "print(f\"    (1.0 = all members of a given class are in the same cluster)\")\n",
    "print(f\"  V-Measure: {v_measure_tw:.4f}\")\n",
    "print(f\"    (harmonic mean of homogeneity and completeness)\")\n",
    "\n",
    "# Show distribution of timezones vs clusters\n",
    "print(f\"\\n  Number of actual timezone classes in sample: {actual_timezones_sample.nunique()}\")\n",
    "print(f\"  Number of DBSCAN clusters found: {len(set(twitter_clusters_sample)) - (1 if -1 in twitter_clusters_sample else 0)}\")\n",
    "\n",
    "# Show some example mappings\n",
    "print(f\"\\n  Sample cluster assignments vs actual timezones:\")\n",
    "sample_df = twitter.loc[twitter_sample_idx, ['timezone', 'dbscan_cluster']].head(20)\n",
    "print(sample_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4f1f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare DBSCAN clusters with actual timezone labels\n",
    "# Note: Only evaluating on the sampled points\n",
    "actual_timezones_sample = twitter.loc[twitter_sample_idx, 'timezone']\n",
    "\n",
    "# Calculate clustering evaluation metrics (on sample only)\n",
    "ari_twitter = adjusted_rand_score(actual_timezones_sample, twitter_clusters_sample)\n",
    "nmi_twitter = normalized_mutual_info_score(actual_timezones_sample, twitter_clusters_sample)\n",
    "homogeneity_tw, completeness_tw, v_measure_tw = homogeneity_completeness_v_measure(actual_timezones_sample, twitter_clusters_sample)\n",
    "\n",
    "print(\"Twitter Dataset - Clustering Evaluation (on sampled data):\")\n",
    "print(f\"  Sample size: {len(twitter_clusters_sample)} points\")\n",
    "print(f\"\\n  Adjusted Rand Index (ARI): {ari_twitter:.4f}\")\n",
    "print(f\"    (1.0 = perfect match, 0.0 = random, negative = worse than random)\")\n",
    "print(f\"\\n  Normalized Mutual Information (NMI): {nmi_twitter:.4f}\")\n",
    "print(f\"    (1.0 = perfect match, 0.0 = no mutual information)\")\n",
    "print(f\"\\n  Homogeneity: {homogeneity_tw:.4f}\")\n",
    "print(f\"    (1.0 = each cluster contains only members of a single class)\")\n",
    "print(f\"  Completeness: {completeness_tw:.4f}\")\n",
    "print(f\"    (1.0 = all members of a given class are in the same cluster)\")\n",
    "print(f\"  V-Measure: {v_measure_tw:.4f}\")\n",
    "print(f\"    (harmonic mean of homogeneity and completeness)\")\n",
    "\n",
    "# Show distribution of timezones vs clusters\n",
    "print(f\"\\n  Number of actual timezone classes in sample: {actual_timezones_sample.nunique()}\")\n",
    "print(f\"  Number of DBSCAN clusters found: {len(set(twitter_clusters_sample)) - (1 if -1 in twitter_clusters_sample else 0)}\")\n",
    "\n",
    "# Show some example mappings\n",
    "print(f\"\\n  Sample cluster assignments vs actual timezones:\")\n",
    "sample_df = twitter.loc[twitter_sample_idx, ['timezone', 'dbscan_cluster']].head(20)\n",
    "print(sample_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f7ad3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze cluster mapping for Twitter (on sampled data only)\n",
    "twitter_sample_data = twitter.loc[twitter_sample_idx]\n",
    "\n",
    "print(\"Cluster Mapping: DBSCAN Cluster → Ground Truth Timezones (Sampled Data)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get unique DBSCAN clusters from sample (excluding noise for main analysis)\n",
    "unique_twitter_clusters = sorted([c for c in twitter_clusters_sample if c != -1])\n",
    "unique_twitter_clusters = list(set(unique_twitter_clusters))[:10]  # Top 10\n",
    "\n",
    "for dbscan_cluster in unique_twitter_clusters:\n",
    "    # Get all samples in this DBSCAN cluster\n",
    "    cluster_data = twitter_sample_data[twitter_sample_data['dbscan_cluster'] == dbscan_cluster]\n",
    "    \n",
    "    # Count ground truth timezones in this cluster\n",
    "    timezone_counts = cluster_data['timezone'].value_counts()\n",
    "    total_in_cluster = len(cluster_data)\n",
    "    \n",
    "    print(f\"\\nDBSCAN Cluster {dbscan_cluster} ({total_in_cluster} samples):\")\n",
    "    print(f\"  Top ground truth timezones:\")\n",
    "    for i, (timezone, count) in enumerate(timezone_counts.head(5).items(), 1):\n",
    "        percentage = (count / total_in_cluster) * 100\n",
    "        print(f\"    {i}. '{timezone}': {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Purity: percentage of most common timezone\n",
    "    if len(timezone_counts) > 0:\n",
    "        purity = (timezone_counts.iloc[0] / total_in_cluster) * 100\n",
    "        print(f\"  Purity: {purity:.1f}% (dominated by '{timezone_counts.index[0]}')\")\n",
    "\n",
    "# Overall cluster purity\n",
    "purity_twitter = calculate_purity(actual_timezones_sample, twitter_clusters_sample)\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(f\"Overall Cluster Purity: {purity_twitter:.4f}\")\n",
    "print(f\"  (Higher is better, 1.0 means each cluster is pure)\")\n",
    "\n",
    "# Show noise cluster statistics\n",
    "noise_count_twitter = (twitter_clusters_sample == -1).sum()\n",
    "if noise_count_twitter > 0:\n",
    "    print(f\"\\nNoise Points (Cluster -1): {noise_count_twitter} samples\")\n",
    "    noise_timezones = twitter_sample_data[twitter_sample_data['dbscan_cluster'] == -1]['timezone'].value_counts()\n",
    "    print(f\"  Top timezones in noise:\")\n",
    "    for timezone, count in noise_timezones.head(5).items():\n",
    "        print(f\"    '{timezone}': {count}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
